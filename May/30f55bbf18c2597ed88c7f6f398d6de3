Facebook said it removed 2.2 billion fake accounts in the first quarter, a record that shows how the company is battling an avalanche of bad actors trying to undermine the authenticity of the world’s largest social network. In the final quarter of 2018, Facebook disabled just over 1 billion fake accounts and 583 million in the first quarter of last year. The vast majority are removed within minutes of being created, the company said, so they’re not counted in Facebook’s closely watched monthly and daily active user metrics. Facebook also shared a new metric in Thursday’s report: The number of posts removed that were promoting or engaging in drug and firearm sales. Facebook pulled more than 1.5 million posts from these categories in the first three months of this year.    Tuesday’s report is a striking reminder of the scale at which Facebook operates - and the size of its problems. The company has been under constant criticism about its content policies and efforts to detect fake accounts since the 2016 US presidential election, when Russia used the social network to try to sway voters. Facebook has promised repeatedly to be better at detecting and removing posts that violate its policies, and has pledged that artificial intelligence programs would be at the center of those efforts. Facebook says it is simply too big to monitor everything on its service with humans alone. The social-media giant released its third ever content transparency report on Thursday, a bi-annual document that outlines Facebook’s efforts to remove posts and accounts that violate its policies. The company said it’s getting better at finding and removing other troubling content like hate speech in the process. Facebook’s AI algorithms work well for some issues, like graphic and violent content. Facebook detects almost 99% of all graphic and violent posts it removes before a user reports them to the company. But they are far from perfect. Facebook still can’t consistently detect graphic or violent content in live videos, for example, a blind spot that allowed a shooter to live broadcast his killing spree at a New Zealand Mosque earlier this year. The software also hasn’t worked as well for more nuanced categories, like hate speech, where context around user relationships and language can be a big factor. Still, Facebook says it’s getting better. Over the past six months, 65% of the posts Facebook removed for pushing hate speech were automatically detected. A year ago, that number was just 38%.