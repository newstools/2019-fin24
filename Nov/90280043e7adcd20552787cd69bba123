I have a strong negative reaction when I find out that law enforcement or a big corporation like Facebook use FRT to identify and even track me. But I love the Face ID feature on my iPhone. Why is it that it bothers me so much in some instances, and I don’t even think twice about other applications? It might be because big corporates and law enforcement tend to use FRT without telling us or explaining why. When we find out that our faces are being monitored, scanned and tracked by companies so that they can better target advertising at us we feel violated and scared. In contrast I chose to switch on and set up the Face ID feature on my iPhone, and I can switch it off at any time. This gives me a (likely false) sense of security.  Is it reasonable for us to freak out a little? Yes, absolutely. The thing with biometric data (think DNA, fingerprints, your face) is that it is extremely private. You can change your password and switch off your computer, but you can’t switch off your face. The questionable history of FRT and how it shapes the technology today FRT is based on the debunked pseudosciences physiognomy (a practice of assessing a person’s character or personality from their outer appearance, especially the face) and phrenology (a practice of measuring bumps on the skull to predict mental traits). Not very long ago these pseudosciences were once used to perpetuate Nazi race ‘science’ and to legitimise slavery. Phrenology was also used to distinguish a ‘genuine husband’ from a ‘unreliable’ husband by the shape of a man’s head. Sounds legit right? Today these pseudosciences have found new life in the application of FRT. A couple of years ago Stanford researchers claimed to have invented an artificial intelligence ‘gaydar’ that could identify gay men with 81 percent accuracy. HireVue sells technology that uses AI to analyse videos of job applicants and scores applicants on measures like ‘personal stability’ and ‘conscientiousness and responsibility’. These are just two of many examples where the use of FRT makes me extremely uncomfortable. But what bothers me even more is the gender identification inaccuracies and racial biases. A researcher at M.I.T Media Lab did a study using the faces of lawmakers from African nations (predominantly dark-skinned residents) and Nordic countries (mainly light-skinned residents). The study showed an error rate for dark-skinned women of 21 percent for Microsoft’s technology and nearly 35 percent for IBM’s technology. The error rate for white skinned men was less than 1 percent. In a test conducted by ACLU Amazon’s FRT  (Rekognition) incorrectly identified 28 members of the US Congress as other people who have been arrested for a crime. If we have nothing to hide, we have nothing to fear right? Not if you’re misidentified and arrested for someone else’s crime! FRT is a Machiavellian approach to security, at best. FTR quickly moved on from profiling and matchmaking to stake its claim as a crime fighting tool. Law enforcement agencies (and opportunists) champion its potential for ‘smart’ surveillance and security. What if you could identify the bad guys and girls in a crowd? Before they act. Despite FRT’s shortcomings, the end justifies the means they say. But at what cost, I ask? Hello there Big Brother 2.0. No, I’m not talking about that ghastly reality show. I’m referring to the Culture of Surveillance we are living in. We’ve seen the images of Hong Kong’s anti-surveillance protestors wearing masks and carrying umbrellas while tearing down ‘smart’ lampposts. And closer to home, there has been numerous concerns about the massive rollout of private camera networks in the City of Johannesburg. Why is this kind of surveillance for security problematic? Accuracy is a huge issue. Earlier this year, Sky News reported that the facial recognition system the London Metropolitan Police uses gets it wrong 81% of the time. Yikes! Then there is the fact that the cameras used in Joburg to curb crime, faces a ban in the United States following the discovery of several cyber vulnerabilities. The kind of weak spots that require very little skill to hack. Imagine information about your daily routine gets into the hands of the very people the system is trying to secure you against. Oh, and then there is the prickly issue of collecting personal data without citizens’ consent. Interestingly, the Pretoria High Court recently declared some sections of South Africa’s interception law (RICA) unconstitutional. The court also found that bulk surveillance and the interception of foreign signals are unlawful and invalid in the absence of a law authorising it. Surprise, surprise. Is FRT regulated? In the US, lawmakers have started introducing rules that bar law enforcement from using FRT to survey everyday citizens. A number of cities, led by San Francisco, have banned government from using the technology. The UK High Court dismissed a challenge against police use of FRT the other week. The court found that even though the police’s use of the technology interferes with individual’s human rights to respect for private and family life, the police did not act in contravention of the UK Data Protection act as it was within the police’s common law powers to prevent and detect crime. As long as the police deployed the technology openly and transparently with significant public engagement, the use of FRT was proportionate. This year a Swedish school board was fined SEK 200 000 (just over R300 000) for using facial recognition to take class register. The Swedish Data Inspection Authority found that the way the school handled leaners’ personal information did not comply with the European General Data Protection Regulation (GDPR). In South Africa we are, of course, still waiting for the Protection of Personal Information Act to become effective. Will the POPIA protect us against unscrupulous use of FRT? We certainly hope so. If the Information Regulator follows the approach we’ve seen in the EU, there will be very limited instances where FRT can be used without explicit consent from the individual. In addition there must be absolute transparency about the where and how the technology is used. Until we have robust regulation in place, I’ll be wearing big hats and sunglasses. Ilze Luttig Hattingh is an attorney and director of Novation Consulting. She specialises in regulatory compliance, risk management, and commercial contract law.